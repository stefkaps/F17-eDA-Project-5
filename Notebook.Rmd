---
title: "Final Project: Global Burden of Disease"
author: 'Group 2: Kelly Jennings, Stefanos Kapetanakis, Marcus Martinez, Rachel Tarrant, Changyong Yi'
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
require(dplyr)
require(MASS)
require(ISLR)
require(ggplot2)
library(leaps)
library(glmnet)
require(tree)
require(randomForest)
require(gbm)
require(data.world)
require(shiny)
require(e1071)
knitr::opts_chunk$set(echo = TRUE)
```
  
## **R Session Info**  

```{r}
sessionInfo()
```

## **Github Link** 
[https://github.com/stefkaps/F17-eDA-Project-5](https://github.com/stefkaps/F17-eDA-Project-5)

## **Data.World Link**
[https://data.world/tarrantrl/f-17-eda-project-5](https://data.world/tarrantrl/f-17-eda-project-5)

## **Connecting to data.world** 
```{r}
project <- "https://data.world/kellyjennings/disease"
data.world::set_config(cfg_env("DW_API"))
df <- data.world::query(
  data.world::qry_sql("SELECT * FROM GlobalBurdenofDisease_Europe"),
  dataset = project
)
```

## Setup
We created several dataframes for our analyses. 
The first thing we did was convert the age to all be in units of years (since some measurements in the dataset were in days). We no longer needed the column age_name_unit after standardizing the age units.
In addition, there were some variables that we excluded from all our analyses because they were not meaningful as predictors. These included cause_medium and cause_short, which were just different versions of the full cause name.
```{r}
df_allyears <- df %>% dplyr::mutate(., age_name_from2 = ifelse(age_name_unit == "days", age_name_from/365, age_name_from)) %>% dplyr::mutate(., age_name_upto2 = ifelse(age_name_upto == "days", age_name_upto/365, age_name_upto)) %>% dplyr::select(., -age_name_upto, -age_name_from,-age_name_unit, -cause_medium, -cause_short)

```
## **Introduction** 
In our final project, we decided to examine a fairly large dataset regarding the global burden of disease. This dataset, provided by a study done by the Insitute for Health Metrics and Evaluation, inluded information on number of deaths, death percent, death rate, years of life lost, years of life lived with a disability, life years adjusted for diability, and the causes of these measures. Throughout our analyses of these data, we generally focused on Europe, rather than the whole dataset (Western and Eastern Europe). However, we looked at a variety of subsets of the data, inlcuding specific years, specific disease types, etc. There was a lot to be learned here, but the areas we gave particular attention to were: 

## Death_abs
Interesting finding: The model based on boosting predictors did somewhat better in terms of adjusted r-squared than a model based on predictors found in both lasso and boosting analyses. The differences are very small, and, in fact, we see that the standard error for the two predictors used in the second model may be smaller than the standard error for the same predictors in the previous model. However, when we only look at r-squared values, we see the following order in terms of performance- Model 1 (yll_abs_ui_from, yll_abs, yll_abs_ui_upto), Random Forest, Model 2 (yll_abs_ui_from, yll_abs).
-Boosting (for this analysis, we excluded categorical variables for ease of computation. We also excluded death_abs_ui_upto and death_abs_ui_from because these are different aspects of the death_abs measurement itself and are therefore not valid predictors of death_abs)
```{r}
df_deathabs=dplyr::select(df_allyears, -cause_name, -region_name, -sex_name, -death_abs_ui_upto, -death_abs_ui_from)
set.seed(1)
train_deathabs = sample(1:nrow(df_deathabs),7000)
#dfb=dplyr::sample_n(dfb, 3000)
boost.deathabs=gbm(death_abs~.,data=df_deathabs[train_deathabs,],distribution="gaussian",n.trees=1000,shrinkage=0.01,interaction.depth=4)
summary(boost.deathabs,plotit=FALSE)
```
This selected yll_abs_ui_from, yll_abs, death_pct_ui_from, daly_abs, and yll_abs_ui_upto as the variables that contributed the most.
```{r} 
# SOMETHING ISN'T RIGHT WITH THIS
df_test_deathabs = df_deathabs[-train_deathabs,]
test_deathabs = sample(1:nrow(df_test_deathabs),7000)

n.trees=seq(from=100,to=1000,by=100)
predmat_deathabs=predict(boost.deathabs,newdata=df_test_deathabs[test_deathabs,],n.trees=n.trees)
summary(predmat_deathabs)

berr_deathabs=with(df_test_deathabs[test_deathabs,],apply( (predmat_deathabs-death_abs)^2,2,mean))
renderPlot({
  plot(n.trees,berr_deathabs,pch=19,ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")
  abline(h=min(berr_deathabs),col="red")
})
```

-Subsequent linear regression
-Backwards/forwards/best subset? (No analysis on predictors gleaned from this. Maybe don't include?)
-Lasso on death abs
-Subsequent updated linear regression (fared worse in terms of adjr2. Intercept doubles practically. std. error actually goes down a lot for predictors though- I previously missed/misinterpreted this. I can make a comment on the post if need be. Or we can just mention it here. Take a look to confirm please. Either way it won't affect insight numbers.)
-Random forest on death_abs (similar r2, sits between original LR and updated LR)

## YLL_abs
Interesting finding: The predictors chosen in both boosting and lasso analyses fared well in linear regression, having a significant p-value for the model and a very high r-squared value. Interstingly, best subset selection, and backwards and forwards stepwise selection show the best models around 2-3 predictors, and then get continually worse from there. We have never seen this particular distribution before, and looked out our analyses for quite a while. We believe this may just be a consequence of the data (possibly due to correlated values). 
-Boosting
-Lasso
-Linear regression
-Best subset selection
-Forwards/backwards
-Random forests

## Eastern vs Western Europe
Interesting finding: Boosting did significantly better than LR, LDA, QDA, or even KNN. K means clustering helps visualize why this might be the case.
- Boosting model
- LR, LDA, QDA, KNN
- K means clustering

## Cause
Because there were so many classes for cause, we did some analysis where we looked at subsets of the causes. This included looking at cancer and heart disease.
- Best subset selection
-KNN

## Unsupervised Learning
-K-means on ischemic heart disease/stroke
-Hierarchical clustering on ischemic heart disease/stroke
Intersting finding: After looking at a scatterplot of yld_rate vs yll_rate for only data regarding Ischemic heart disease and stroke in Europe, we noticed a natural and cleare separation between Western and Eastern Europe. Interestingly, K-means clustering didn't separate the data as we would have expected it to, which was possibly due to one of the centroids used by the analysis being focused in a highly clustered area (data points from both regious were very close together). Data around the other centroid had points that were mostly belonging to Eastern Europe, which was mostly correct. Had our data been a bit more spread out, K-means clustering may have done a better job. Compared to K-means clustering, hierarchical clustering (complete, single, and average) did a much worse job. Overall, the closest Hierarchical came to K-means results (which were already not perfect), was using the complete method. 
-PCA on subset of data
Intersting finding: The predictors displayed in the biplot of this analysis were clustered by type (death rate predictors grouped together, years of life lost predictrors grouped together, etc.). This makes sense, conisdering they are usually pretty standard variations of each other. (Need to further interpret this biplot.)

## Insights
[First Attempt at Best Subset Selection Predicting Cause]https://data.world/tarrantrl/f-17-eda-project-5/insights/01a30c5e-ac21-4b5e-867c-5ddc2f7e5e21
[KNN on Cause]https://data.world/tarrantrl/f-17-eda-project-5/insights/473800c5-aa3e-4d22-9110-231e17944d4d
[Pairs charts between subsets of predictors]https://data.world/tarrantrl/f-17-eda-project-5/insights/a1cd6a73-b94d-4c0e-af4b-3fa5beb40834
[Boosting to find important predictors for death_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/26c94e87-2611-4bcc-b851-6d938ff1e45d
[Multi-predictor Linear Regression Model for death_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/2c1db0df-9dae-4b4c-a816-d862f903a179
[Boosting to Find Best Predictors of yll_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/743d8695-b85b-49ac-b537-05f0b554b809
[Backwards, Forwards, and Best Subset Selection for death_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/3350db18-efb4-4359-92bd-c64b0f3274f5
[Lasso on death_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/2d587ac9-73ff-49ec-9b48-1094cca839b1
[Linear Regression on death_abs with Updated Predictors]https://data.world/tarrantrl/f-17-eda-project-5/insights/b4a21227-5c31-4120-9b0e-0c2e7207c9fc
[Lasso Analysis on yll_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/5ff96be6-c362-4ae9-bd61-6e5c9409b0da
[K means clustering on Ischemic Heart Disease and Stroke]https://data.world/tarrantrl/f-17-eda-project-5/insights/c055cd89-0846-444a-a63c-bcf5c80f7746
[Linear Regression on yll_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/8593766c-f3de-4a3d-a054-a91dc9130785
[Hierarchical Clustering on Ischemic Heart Disease and Stroke]https://data.world/tarrantrl/f-17-eda-project-5/insights/d8c6a10b-e3f5-4e5d-ba96-07aa2dc164f8
[Cause of death]https://data.world/tarrantrl/f-17-eda-project-5/insights/1c0d5968-d862-429d-bc27-77f72e3a9f73
[Lasso on region_name]https://data.world/tarrantrl/f-17-eda-project-5/insights/fffa3067-a1e1-451a-8159-e1aeb1a0bb79
[PCA on Subset of the Data]https://data.world/tarrantrl/f-17-eda-project-5/insights/03e1d5df-1b45-4ba9-a218-ceef43276330
[Boosting to predict cancer class]https://data.world/tarrantrl/f-17-eda-project-5/insights/4f220f02-ae27-4095-9491-19724fb702fc
[Linear SVM on Yld Rate ~ Death Rate for Multiple Sclerosis]https://data.world/tarrantrl/f-17-eda-project-5/insights/b04d895b-d6a9-494d-80cd-ed9728a1f634
[Radial SVM on Yld Rate ~ Death Rate for Multiple Sclerosis]https://data.world/tarrantrl/f-17-eda-project-5/insights/79072de3-484f-45cc-aa74-82f8cfda6589
[Random Forest on death_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/230157c7-8aed-4067-b57f-6b8bf34ab6f6
[Boosting to distinguish eastern and western Europe]https://data.world/tarrantrl/f-17-eda-project-5/insights/d0b1119b-03e3-492b-8389-1ade40af2b55
[Best Subset Selection for yll_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/e85e9b69-9203-4244-ad1d-a543c6c76036
[Logistic regression for region]https://data.world/tarrantrl/f-17-eda-project-5/insights/8cd11f42-8408-44ba-8599-01f84a6c5627
[Forward & Backwards Selection of yll_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/32178177-a72b-4064-a6e2-07a6422034e9
[LDA and QDA for region classification]https://data.world/tarrantrl/f-17-eda-project-5/insights/9e6d23b8-58be-4148-a19f-0c5dbacda6c9
[Boosting to distinguish Male and Female]https://data.world/tarrantrl/f-17-eda-project-5/insights/45e1626b-fdfc-4543-90f1-61b2dae599af
[Random Forest on yll_abs]https://data.world/tarrantrl/f-17-eda-project-5/insights/e55cacf1-7100-42da-90bf-6bc2df4a3a0a
[LDA to predict sex]https://data.world/tarrantrl/f-17-eda-project-5/insights/546d9d99-645b-4454-b330-a1064e03b02f
[ROC curve for prediction of sex]https://data.world/tarrantrl/f-17-eda-project-5/insights/179ec2db-7e86-4e53-b915-27a6df1c4eea
[ROC curve for predicting region]https://data.world/tarrantrl/f-17-eda-project-5/insights/329b7f64-fbfc-47e0-9190-dc770ef387a1
[Boosting for region revisited with error]https://data.world/tarrantrl/f-17-eda-project-5/insights/4fa586dd-90ff-4139-bdd5-1833c48e1b00
[KNN for region]https://data.world/tarrantrl/f-17-eda-project-5/insights/8f97ddc5-c6e4-405d-8168-c895a40381dd
[K means clustering for region]https://data.world/tarrantrl/f-17-eda-project-5/insights/abcaf28b-e8ca-416f-a41a-6322507d52c8
